{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840e5cc6",
   "metadata": {},
   "source": [
    "# 3.2 Applying LabelAId to Project Sidewalk\n",
    "\n",
    "This notebook demonstrates how to apply programmatic weak supervision (PWS) for Project Sidewalk data labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn import metrics\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import seaborn as sns\n",
    "\n",
    "# Snorkel-specific imports for programmatic labeling\n",
    "import snorkel\n",
    "from snorkel.labeling import labeling_function, LFAnalysis, PandasLFApplier\n",
    "from snorkel.labeling.model import MajorityLabelVoter, LabelModel\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.utils import probs_to_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4b86cc",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "Load the dataset and perform any necessary preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966def5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test datasets with one-hot encoded features\n",
    "df_train = pd.read_csv(\"../data/training_set_seattle_encoded.csv\")\n",
    "df_test = pd.read_csv('../data/test_set_seattle_encoded.csv')\n",
    "\n",
    "# Preview the data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5114b",
   "metadata": {},
   "source": [
    "## 3.2.2 Define Labeling Functions\n",
    "Labeling functions are heuristics used to generate noisy labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f18857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label mappings for convenience\n",
    "NOT_SURE = -1\n",
    "WRONG = 0\n",
    "CORRECT = 1\n",
    "\n",
    "@labeling_function()\n",
    "def distance_to_intersection(x):\n",
    "    \"\"\"\n",
    "    Labels as WRONG if the distance to the intersection falls within certain thresholds\n",
    "    for residential streets and living streets; otherwise, NOT_SURE.\n",
    "    \"\"\"\n",
    "    if x[\"way_residential\"] and (80 <= x[\"distance_to_intersection\"] or 0 < x[\"distance_to_intersection\"] <= 5):\n",
    "        return WRONG \n",
    "    elif x[\"way_living_street\"] and (50 <= x[\"distance_to_intersection\"] or 0 < x[\"distance_to_intersection\"] <= 5):\n",
    "        return WRONG\n",
    "    return NOT_SURE\n",
    "\n",
    "@labeling_function()\n",
    "def clustered(x):\n",
    "    \"\"\"\n",
    "    Labels as CORRECT if the label type matches specific conditions and the count of similar labels\n",
    "    in a cluster is above a threshold; otherwise, NOT_SURE.\n",
    "    \"\"\"\n",
    "    if x[\"label_type\"] =='NoSidewalk' and x[\"cluster_label_count\"] >= 2:\n",
    "        return CORRECT\n",
    "    elif x[\"label_type\"] =='NoCurbRamp' and x[\"cluster_label_count\"] >= 2:\n",
    "        return CORRECT \n",
    "    elif x[\"label_type\"] =='CurbRamp' and x[\"cluster_label_count\"] >= 2:\n",
    "        return CORRECT\n",
    "    else:\n",
    "        return NOT_SURE\n",
    "\n",
    "@labeling_function()\n",
    "def severity(x):\n",
    "    \"\"\"\n",
    "    Assigns WRONG for certain types below a severity threshold, and CORRECT for others above a\n",
    "    severity threshold; otherwise, NOT_SURE.\n",
    "    \"\"\"\n",
    "    if x[\"label_type\"] == 'NoSidewalk' and x[\"severity\"] < 3:\n",
    "        return WRONG\n",
    "    elif x[\"label_type\"] == 'NoCurbRamp' and x[\"severity\"] <= 2:\n",
    "        return WRONG\n",
    "    elif x[\"label_type\"] == 'CurbRamp' and x[\"severity\"] >= 4:\n",
    "        return WRONG\n",
    "    elif x[\"label_type\"] in ['Obstacle', 'SurfaceProblem'] and x[\"severity\"] >= 4:\n",
    "        return CORRECT\n",
    "    return NOT_SURE\n",
    "\n",
    "@labeling_function()\n",
    "def zoom(x):\n",
    "    \"\"\"\n",
    "    Labels as CORRECT if zoom level is above 1; assigns WRONG under specific conditions; otherwise, NOT_SURE.\n",
    "    \"\"\"\n",
    "    if x[\"zoom\"] > 1:\n",
    "        return CORRECT\n",
    "    elif x[\"label_type\"] in ['Obstacle', 'SurfaceProblem'] and x[\"zoom\"] == 1:\n",
    "        return WRONG\n",
    "    return NOT_SURE\n",
    "\n",
    "@labeling_function()\n",
    "def tags(x):\n",
    "    \"\"\"\n",
    "    Labels as CORRECT if label type is among specific types and a tag is present; otherwise, NOT_SURE.\n",
    "    \"\"\"\n",
    "    if x[\"label_type\"] in ['NoSidewalk', 'NoCurbRamp', 'CurbRamp'] and x[\"tag\"] == 1:\n",
    "        return CORRECT\n",
    "    return NOT_SURE\n",
    "\n",
    "@labeling_function()\n",
    "def description(x):\n",
    "    \"\"\"\n",
    "    Labels as CORRECT if a description is present; otherwise, NOT_SURE.\n",
    "    \"\"\"\n",
    "    return CORRECT if x['description'] == 1 else NOT_SURE\n",
    "\n",
    "@labeling_function()\n",
    "def distance_to_road(x):\n",
    "    \"\"\"\n",
    "    Labels as WRONG if the distance to road is below or above certain thresholds based on the way type;\n",
    "    otherwise, NOT_SURE.\n",
    "    \"\"\"\n",
    "    thresholds = {\"way_residential\": 40, \"way_primary\": 60, \"way_secondary\": 50, \"way_tertiary\": 45}\n",
    "\n",
    "    if x[\"label_type\"] in ['Obstacle', 'SurfaceProblem']:\n",
    "        if x[\"distance_to_road\"] < 10:\n",
    "            return WRONG\n",
    "        for way_type, threshold in thresholds.items():\n",
    "            if x[way_type] and x[\"distance_to_road\"] > threshold:\n",
    "                return WRONG\n",
    "    return NOT_SURE\n",
    "\n",
    "@labeling_function()\n",
    "def way_type(x):\n",
    "    \"\"\"\n",
    "    Labels as CORRECT or WRONG based on the unclassified way type and the label type;\n",
    "    otherwise, NOT_SURE.\n",
    "    \"\"\"\n",
    "    if x[\"way_unclassified\"]:\n",
    "        if x[\"label_type\"] == \"CurbRamp\":\n",
    "            return CORRECT\n",
    "        elif x[\"label_type\"] in [\"Obstacle\", \"SurfaceProblem\", \"NoSidewalk\"]:\n",
    "            return WRONG\n",
    "    return NOT_SURE\n",
    "\n",
    "lfs = [\n",
    "    distance_to_intersection, \n",
    "    clustered,\n",
    "    severity,\n",
    "    zoom,\n",
    "    tags,\n",
    "    description,\n",
    "    distance_to_road,\n",
    "    way_type\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3333efb",
   "metadata": {},
   "source": [
    "## Apply Labeling Functions to Data\n",
    "Apply the defined labeling functions to create a label matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af051d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the labeling function applier using the Pandas DataFrame.\n",
    "# This applier uses the labeling functions defined above to annotate the DataFrame.\n",
    "lf_applier = PandasLFApplier(lfs=lfs)\n",
    "\n",
    "# Apply the labeling functions to the training data.\n",
    "# This step generates a label matrix where each row corresponds to a data point,\n",
    "# and each column corresponds to the output of a labeling function.\n",
    "L_train = lf_applier.apply(df=df_train)\n",
    "\n",
    "# Analyze the results of the labeling functions.\n",
    "# The LFAnalysis utility provides statistics on the coverage, overlaps, conflicts, and more,\n",
    "# helping to understand how the labeling functions are performing on the dataset.\n",
    "lf_summary = LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06148e6",
   "metadata": {},
   "source": [
    "## Train the Label Model\n",
    "Train a Snorkel Label Model to aggregate the labeling functions outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Label Model with specified parameters.\n",
    "# Refer to https://snorkel.readthedocs.io/en/v0.9.3/packages/_autosummary/labeling/snorkel.labeling.LabelModel.html for more details of hyperparameters.\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "\n",
    "# Fit the Label Model on the training data.\n",
    "label_model.fit(L_train=L_train, n_epochs=500, lr=0.001, log_freq=100, seed=14, lr_scheduler='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities_histogram(Y):\n",
    "    \"\"\"\n",
    "    Plots a histogram of probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "        Y (array-like): An array of probabilities for each data point.\n",
    "        \n",
    "    This function plots the histogram of the probabilities that each data point has been classified as CORRECT.\n",
    "    The x-axis represents the probability of being correct, and the y-axis shows the number of data points\n",
    "    with that probability.\n",
    "    \"\"\"\n",
    "    plt.hist(Y, bins=10)  # Plot the histogram with 10 bins\n",
    "    plt.xlabel('Probability of being CORRECT')\n",
    "    plt.ylabel('Number of data points')\n",
    "    plt.show()\n",
    "\n",
    "# Predict probabilities using the trained label model\n",
    "probs_train = label_model.predict_proba(L=L_train)\n",
    "\n",
    "plot_probabilities_histogram(probs_train[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688e395",
   "metadata": {},
   "source": [
    "## Save PWS Outputs\n",
    "Save the predictions and probabilities from PWS pipeline for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dbd0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(X=df_train, y=probs_train, L=L_train)\n",
    "\n",
    "# Convert the probabilities to predictions (the target variable)\n",
    "preds_train_filtered = probs_to_preds(probs=probs_train_filtered)\n",
    "\n",
    "# Merge the filtered training data with predictions and the probabilities of being correct\n",
    "merged = pd.concat([\n",
    "    df_train_filtered,  # The filtered data\n",
    "    pd.DataFrame(preds_train_filtered, index=df_train_filtered.index, columns=['Hard_Label']),  # Add predictions\n",
    "    pd.DataFrame(probs_train_filtered[:,1], index=df_train_filtered.index, columns=['Soft_Probability_of_Correct'])  # Add probabilities\n",
    "], axis=1)\n",
    "\n",
    "# Save the merged data to a CSV file\n",
    "merged.to_csv('PWS_outputs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse446",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
