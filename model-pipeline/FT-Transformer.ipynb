{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying LabelAId to Project Sidewalk (FT-Transformer)\n",
    "\n",
    "This notebook demonstrates how to apply FT-Transformer after programmatic weak supervision (PWS) to infer Project Sidewalk label accurary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "FT-Transformer Model Github Repo: https://github.com/aruberts/TabTransformerTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tabtransformertf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "\n",
    "# Encoder is specified separately in case we decide to pre-train the model\n",
    "ft_linear_encoder = FTTransformerEncoder(\n",
    "    numerical_features = NUMERIC_FEATURES, # list of numeric features\n",
    "    categorical_features = CATEGORICAL_FEATURES, # list of categorical features\n",
    "    numerical_data = X_train[NUMERIC_FEATURES].values, # train array of numerical features\n",
    "    categorical_data = X_train[CATEGORICAL_FEATURES].values, # train array of categorical features\n",
    "    y = None, # not needed for linear\n",
    "    numerical_embedding_type='linear',\n",
    "    embedding_dim=16,  # Embedding dimension (for categorical, numerical, and contextual)\n",
    "    depth=3,  # Number of Transformer Blocks (layers)\n",
    "    heads=6,  # Number of attention heads in a Transofrmer Block\n",
    "    attn_dropout=0.2,  # Dropout for attention layers\n",
    "    ff_dropout=0.2,  # Dropout in Dense layers\n",
    "    use_column_embedding=True,  # Fixed column embeddings\n",
    "    explainable=True  # Whether we want to output attention importances or not\n",
    ")\n",
    "\n",
    "# Pass the encoder to the model\n",
    "ft_linear_transformer = FTTransformer(\n",
    "    encoder=ft_linear_encoder,  # Encoder from above\n",
    "    out_dim=1,  # Number of outputs in final layer\n",
    "    out_activation='sigmoid',  # Activation function for final layer\n",
    ")\n",
    "\n",
    "preds = ft_linear_transformer.predict(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse-547",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
